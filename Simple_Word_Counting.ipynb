{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 01\n",
    "## Simple word-counting \n",
    "## TF-IDF semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sys to read files\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import string\n",
    "import re\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "# nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List downloaded files\n",
    "\n",
    "# os.listdir('Data/reuters21578/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading the README text, I know that the collection I am interested in is in the 22 sgm files. \n",
    "To access these files, I will use `open as infile`. \n",
    "\n",
    "There are 6 files describing the categories used to index the data as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening just one file as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a first file as an example. Used 'r' because it is text mode parsing (read only).\n",
    "with open('Data/reuters21578/reut2-008.sgm', 'r', encoding = 'utf-8', errors = 'ignore') as infile:\n",
    "        data = infile.read()\n",
    "\n",
    "# To print as a sanity check, uncomment.\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the difficult format presentation, the corpus is there. \n",
    "\n",
    "I will open all the files in the directory and assemble them in an array called data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE lewis SYSTEM \"lewis.dtd\">\\n<REUTERS TOPICS=\"YES\" LEWISSPLIT=\"TRAIN\" CGISPLIT=\"TRAINING-SET\" OLDID=\"5544\" NEWID=\"1\">\\n<DATE>26-FEB-1987 15:01:01.79</DATE>\\n<TOPICS><D>cocoa</D></TOPICS>\\n<PLACES><D>el-salvador</D><D>usa</D><D>uruguay</D></PLACES>\\n<PEOPLE></PEOPLE>\\n<ORGS></ORGS>\\n<EXCHANGES></EXCHANGES>\\n<COMPANIES></COMPANIES>\\n<UNKNOWN> \\n&#5;&#5;&#5;C T\\n&#22;&#22;&#1;f0704&#31;reute\\nu f BC-BAHIA-COCOA-REVIEW   02-26 0105</UNKNOWN>\\n<TEXT>&#2;\\n<TITLE>BAHIA COCOA REVIEW</TITLE>\\n<DATELINE>    SALVADOR, Feb 26 - </DATELINE><BODY>Showers continued throughout the week in\\nthe Bahia cocoa zone, alle'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for i in range(22):\n",
    "    # Open all filenames. Pad {0} to 3 digits with str methods. \n",
    "    # Use range from 0 to 22.\n",
    "    filename = 'Data/reuters21578/reut2-{0}.sgm'.format(str(i).zfill(3))\n",
    "    \n",
    "    # Encoding with most common scheme.\n",
    "    with open(filename, 'r', encoding = 'utf-8', errors = 'ignore') as infile:\n",
    "        data.append(infile.read())\n",
    "        \n",
    "# Print first 100 characters of the first article\n",
    "data[0][:600]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All corpora have been set up in an array of arrays. We can see that the text format seems to have a HTML presentation. In order to give some format, I will use BeautifulSoup package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, I will see what the NLTK package has for 'Reuters' database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.corpus.reuters.raw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it is the same database, however the articles are in different order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will use BeautifulSoup to parse the text. BeautifulSoup will allow to remove all the HTML tags. There are several ways to do this. I could run a loop and attach an article between the tags of <Body> to a dataframe. (Might do that later - for simplicity now, I will just put it in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "example_soup = BeautifulSoup(data[0], 'html.parser')\n",
    "\n",
    "# print(example_soup.prettify()) # Makes the above easier to read. More as we would see on a describe HTML panel (when scrapping).\n",
    "# print(example_soup.get_text())  # Removes HTML flags.\n",
    "# print(example_soup.find_all('body')) # Extracts the text from the HTML tag we choose. This case, the 'Body' tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now put all the corpus in the soup. The soup object can just take one index at a time. We need to iterate over our data list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<body>Huge oil platforms dot the Gulf like\n",
      "beacons -- usually lit up like Christmas trees at night.\n",
      "    One of them, sitting astride the Rostam offshore oilfield,\n",
      "was all but blown out of the water by U.S. Warships on Monday.\n",
      "    The Iranian platform, an unsightly mass of steel and\n",
      "concrete, was a three-tier structure rising 200 feet (60\n",
      "metres) above the warm waters of the Gulf until four U.S.\n",
      "Destroyers pumped some 1,000 shells into it.\n",
      "    The U.S. Defense Department said just 10 pct of one section\n",
      "of the structure remained.\n",
      "    U.S. helicopters destroyed three Iranian gunboats after an\n",
      "American helicopter came under fire earlier this month and U.S.\n",
      "forces attacked, seized, and sank an Iranian ship they said had\n",
      "been caught laying mines.\n",
      "    But Iran was not deterred, according to U.S. defense\n",
      "officials, who said Iranian forces used Chinese-made Silkworm\n",
      "missiles to hit a U.S.-owned Liberian-flagged ship on Thursday\n",
      "and the Sea Isle City on Friday.\n",
      "    Both ships were hit in the territorial waters of Kuwait, a\n",
      "key backer of Iraq in its war with Iran.\n",
      "    Henry Schuler, a former U.S. diplomat in the Middle East\n",
      "now with CSIS said Washington had agreed to escort Kuwaiti\n",
      "tankers in order to deter Iranian attacks on shipping.\n",
      "    But he said the deterrence policy had failed and the level\n",
      "of violence and threats to shipping had increased as a result\n",
      "of U.S. intervention and Iran's response.\n",
      "    The attack on the oil platform was the latest example of a\n",
      "U.S. \"tit-for-tat\" policy that gave Iran the initiative, said\n",
      "Harlan Ullman, an ex-career naval officer now with CSIS.\n",
      "    He said with this appraoch America would suffer \"the death\n",
      "of one thousand cuts.\"\n",
      "    But for the United States to grab the initiative\n",
      "militarily, it must take warlike steps such as mining Iran's\n",
      "harbors or blockading the mouth of the Gulf through which its\n",
      "shipping must pass, Schuler said.\n",
      "    He was among those advocating mining as a means of bringing\n",
      "Iran to the neogtiating table. If vital supplies were cut off,\n",
      "Tehran could not continue the war with Iraq.\n",
      "    Ullman said Washington should join Moscow in a diplomatic\n",
      "initiative to end the war and the superpowers should impose an\n",
      "arms embargo against Tehran if it refused to negotiate.\n",
      "    He said the United States should also threaten to mine and\n",
      "blockade Iran if it continued fighting and must press Iraq to\n",
      "acknowledge responsibility for starting the war as part of a\n",
      "settlement.\n",
      "    Iranian and Western diplomats say Iraq started the war by\n",
      "invading Iran's territory in 1980. Iraq blames Iran for the\n",
      "outbreak of hostilities, which have entailed World War I-style\n",
      "infantry attacks resulting in horrific casualties.\n",
      "    Each side has attacked the others' shipping.\n",
      " Reuter\n",
      "\u0003</body>\n"
     ]
    }
   ],
   "source": [
    "corpora = []\n",
    "for text in data:\n",
    "    # Parse text as html using beautiful soup\n",
    "    parsed_text = BeautifulSoup(text, 'html.parser')\n",
    "    table = parsed_text.find_all('body')[0] \n",
    "print(table)\n",
    "    #df = pd.read_html(str(table))\n",
    "    #print( tabulate(df[0], headers='keys', tablefmt='psql') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showers continued throughout the week in\n",
      "the Bahia cocoa zone, alleviating the drought since early\n",
      "January and improving prospects for the coming temporao,\n",
      "although normal humidity levels have not been restored,\n",
      "Comissaria Smith said in its weekly review.\n",
      "    The dry period means the temporao will b\n"
     ]
    }
   ],
   "source": [
    "corpora = []\n",
    "for text in data:\n",
    "    # Parse text as html using beautiful soup\n",
    "    parsed_text = BeautifulSoup(text, 'html.parser')\n",
    "    \n",
    "    # Extract article between <BODY> and </BODY> and convert to standard text. Add to list of articles\n",
    "    corpora += [corpora.get_text() for corpora in parsed_text.find_all('body')]\n",
    "# print the first article as an example\n",
    "print(corpora[0][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "After removing the HTML tags, there are still some items that might need further normalization. \n",
    "These tasks I can think of as now are: lowering capital letters, removing punctuation signs and digits and changing vocabulary to their lemmas or stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary where punctuation is mapped to none.\n",
    "no_punc = str.maketrans('', '', string.punctuation) \n",
    "\n",
    "# Remove punctuation from corpora.\n",
    "corpora = [corpus.translate(no_punc) for corpus in corpora]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase all capital letters.\n",
    "corpora = [corpus.lower() for corpus in corpora]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove digits from corpora.\n",
    "corpora = [re.sub(r'\\d+', '', corpus) for corpus in corpora]\n",
    " \n",
    "# Set English and identified/additional stopwords in order to remove them.\n",
    "stopwords = set(nltk.corpus.stopwords.words('english') + ['reuter', '\\x03', '``','’', '`','br','\"',\"”\", \"''\", \"'s\", \"\\\\n\"])\n",
    "corpora = [[word for word in corpus.split() if word not in stopwords] for corpus in corpora]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['showers',\n",
       " 'continued',\n",
       " 'throughout',\n",
       " 'week',\n",
       " 'bahia',\n",
       " 'cocoa',\n",
       " 'zone',\n",
       " 'alleviating',\n",
       " 'drought',\n",
       " 'since']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'continued'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = wnl.lemmatize('continued')\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shower continued throughout week bahia cocoa zone alleviating drought since early january improving prospect coming temporao although normal humidity level restored comissaria smith said weekly review dry period mean temporao late year arrival week ended february bag kilo making cumulative total season mln stage last year seems cocoa delivered earlier consignment included arrival figure comissaria smith said still doubt much old crop cocoa still available harvesting practically come end total bahia crop estimate around mln bag sale standing almost mln hundred thousand bag still hand farmer middleman exporter processor doubt much cocoa would fit export shipper experiencing dificulties obtaining bahia superior certificate view lower quality recent week farmer sold good part cocoa held consignment comissaria smith said spot bean price rose cruzados per arroba kilo bean shipper reluctant offer nearby shipment limited sale booked march shipment dlrs per tonne port named new crop sale also light open port junejuly going dlrs dlrs new york july augsept dlrs per tonne fob routine sale butter made marchapril sold dlrs aprilmay butter went time new york may junejuly dlrs augsept dlrs time new york sept octdec dlrs time new york dec comissaria smith said destination u covertible currency area uruguay open port cake sale registered dlrs marchapril dlrs may dlrs aug time new york dec octdec buyer u argentina uruguay convertible currency area liquor sale limited marchapril selling dlrs junejuly dlrs time new york july augsept dlrs time new york sept octdec time new york dec comissaria smith said total bahia sale currently estimated mln bag crop mln bag crop final figure period february expected published brazilian cocoa trade commission carnival end midday february\n"
     ]
    }
   ],
   "source": [
    "# Change the corpora's full words for just the lemmas\n",
    "wnl = WordNetLemmatizer()\n",
    "corpora_lem = [\" \".join([wnl.lemmatize(word) for word in corpus]) for corpus in corpora]\n",
    "# print the first article as a running example\n",
    "print(corpora_lem[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shower continu throughout week bahia cocoa zone allevi drought sinc earli januari improv prospect come temporao although normal humid level restor comissaria smith said weekli review dri period mean temporao late year arriv week end februari bag kilo make cumul total season mln stage last year seem cocoa deliv earlier consign includ arriv figur comissaria smith said still doubt much old crop cocoa still avail harvest practic come end total bahia crop estim around mln bag sale stand almost mln hundr thousand bag still hand farmer middlemen export processor doubt much cocoa would fit export shipper experienc dificulti obtain bahia superior certif view lower qualiti recent week farmer sold good part cocoa held consign comissaria smith said spot bean price rose cruzado per arroba kilo bean shipper reluct offer nearbi shipment limit sale book march shipment dlr per tonn port name new crop sale also light open port junejuli go dlr dlr new york juli augsept dlr per tonn fob routin sale butter made marchapril sold dlr aprilmay butter went time new york may junejuli dlr augsept dlr time new york sept octdec dlr time new york dec comissaria smith said destin us covert currenc area uruguay open port cake sale regist dlr marchapril dlr may dlr aug time new york dec octdec buyer us argentina uruguay convert currenc area liquor sale limit marchapril sell dlr junejuli dlr time new york juli augsept dlr time new york sept octdec time new york dec comissaria smith said total bahia sale current estim mln bag crop mln bag crop final figur period februari expect publish brazilian cocoa trade commiss carniv end midday februari\n"
     ]
    }
   ],
   "source": [
    "# Change the corpora's full words for just the stems\n",
    "ps = nltk.PorterStemmer()\n",
    "corpora_stem = [\" \".join([ps.stem(word) for word in corpus]) for corpus in corpora]\n",
    "# print the first article as a running example\n",
    "print(corpora_stem[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
